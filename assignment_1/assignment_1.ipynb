{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Copy of assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaZhuravleva/compling/blob/master/assignment_1/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7LXW09O25cW",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
        "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
        "2. Tokenize text by BPE with vocab_size = 100\n",
        "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
        "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
        "5. Calculate perplexity of the language model for the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0WD6tGX25cY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "import collections\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from sklearn.base import TransformerMixin\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFz4R5wS4PVy",
        "colab_type": "code",
        "outputId": "cf9caffc-5f3c-438d-c700-ba59f3ab8c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "text = open('/content/drive/My Drive/Colab Notebooks/compling/assignment_1/peace.txt')\n",
        "# text = open('peace.txt', 'r', encoding='utf-8-sig')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSr1Joz125cb",
        "colab_type": "code",
        "outputId": "7806aed3-3c50-41d9-8222-5eedaa493564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = text.read()\n",
        "len(text)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3227581"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FSDV_zbZ25ce",
        "colab_type": "code",
        "outputId": "0ad6562b-aa16-45d0-d8f9-3e19fc4876dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def preprocess_text(text):\n",
        "    punct = list(set(re.findall('\\W', text)))\n",
        "    punct.remove('.')\n",
        "    words = ''\n",
        "    for word in text.split():\n",
        "        for letter in word:\n",
        "            if letter not in punct:\n",
        "                words += letter\n",
        "        words += ' '\n",
        "    text = words.lower()\n",
        "        \n",
        "    # replace all punctuation except dots with spaces\n",
        "    # collapse multiple spaces into one '   ' -> ' '\n",
        "    return text\n",
        "\n",
        "\n",
        "text = preprocess_text(text)\n",
        "len(text)\n",
        "# assert len(text) == 3141169"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3130570"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63n1ZAHN25cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.split('.')\n",
        "text = [x.strip() for x in text]\n",
        "text = [t + '.' for t in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BypVTf1z25ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BPE(TransformerMixin):\n",
        "    \n",
        "    def __init__(self, vocab_size=100):\n",
        "        super(BPE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # index to token\n",
        "        self.itos = []\n",
        "        # token to index\n",
        "        self.stoi = {}\n",
        "        \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        fit itos and stoi\n",
        "        text: list of strings \n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO\n",
        "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
        "        self.itos = ['.']\n",
        "        for item in set([item for txt in text for item in set(txt) if item != '.']):\n",
        "          self.itos.append(item)\n",
        "        self.stoi = {l: idx for idx, l in enumerate(self.itos)}\n",
        "        text = [[self.stoi[letter] for letter in txt] for txt in text]\n",
        "        \n",
        "        while len(self.itos) < self.vocab_size:\n",
        "            # TODO\n",
        "            # count bigram freqencies in the text\n",
        "            bigrams = collections.Counter()\n",
        "            for txt in text:\n",
        "                i = 0\n",
        "                while i + 1 < len(txt):\n",
        "                    bigrams[(txt[i], txt[i+1])] += 1\n",
        "                    i += 1\n",
        "                \n",
        "                \n",
        "            mc = bigrams.most_common(1)[0][0]\n",
        "            new_token = str(self.itos[int(mc[0])]) + str(self.itos[int(mc[1])])\n",
        "            new_id = len(self.itos)\n",
        "            \n",
        "            self.itos.append(new_token)\n",
        "            self.stoi[new_token] = new_id\n",
        "            \n",
        "            # find occurences of the new_token in the text and replace them with new_id\n",
        "            tmp = []\n",
        "            for txt in text:\n",
        "                tmp2 = []\n",
        "                i = 0\n",
        "                while i + 1 < len(txt):\n",
        "                    if new_token == self.itos[txt[i]] + self.itos[txt[i+1]]:\n",
        "                        tmp2.append(new_id)\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        tmp2.append(txt[i])\n",
        "                        i += 1\n",
        "                tmp.append(tmp2)\n",
        "                    \n",
        "            text = tmp \n",
        "           \n",
        "        return self\n",
        "    \n",
        "    def transform(self, text):\n",
        "        \"\"\"\n",
        "        convert text to a sequence of token ids\n",
        "        text: list of strings\n",
        "        \"\"\" \n",
        "        max_size = max([len(tok) for tok in self.itos])\n",
        "        \n",
        "        new_text = []\n",
        "        for txt in text:\n",
        "            i = 0\n",
        "            new_txt = []\n",
        "            while i < len(txt):\n",
        "                hit = False\n",
        "                stop = i+max_size if len(txt) - (i + max_size) >= 0 else len(txt)\n",
        "                while hit == False and stop > i:\n",
        "                    if txt[i:stop] in self.itos:\n",
        "                        new_txt.append(self.stoi[txt[i:stop]])\n",
        "                        hit = True\n",
        "                        i = stop\n",
        "                    else:\n",
        "                        stop -= 1\n",
        "            new_text.append(new_txt)\n",
        "                                \n",
        "        text = new_text\n",
        "       # for token_id, token in enumerate(self.itos):\n",
        "            # find occurences of the token in the text and replace them with token_id\n",
        "          #  text = # TODO       \n",
        "        return text\n",
        "    \n",
        "    def decode_token(self, tok):\n",
        "        \"\"\"\n",
        "        tok: int or tuple\n",
        "        \"\"\"\n",
        "        result = self.itos[tok] if isinstance(tok, int) else [self.itos[i] for i in token]\n",
        "        return result\n",
        "            \n",
        "    def decode(self, text):\n",
        "        \"\"\"\n",
        "        convert token ids into text\n",
        "        \"\"\"\n",
        "        return ''.join(map(self.decode_token, text))\n",
        "        \n",
        "        \n",
        "vocab_size = 100\n",
        "bpe = BPE(vocab_size)\n",
        "tokenized_text = bpe.fit_transform(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X7yE3-R25ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert bpe.decode(tokenized_text[0]) == text[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWqPZVe625cm",
        "colab_type": "code",
        "outputId": "2d61ef6b-46f7-4f66-87b9-c1adf40556a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "bpe.decode(tokenized_text[0]) "
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the project gutenberg ebook of war and peace by leo tolstoy this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRJJ9agL25co",
        "colab_type": "code",
        "outputId": "199ba658-74a1-410d-82fb-35326fdd4d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "text[0]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the project gutenberg ebook of war and peace by leo tolstoy this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zb1Xetj25cq",
        "colab_type": "code",
        "outputId": "eb46944a-54ed-4eae-b57a-a5e52fcfcfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print(tokenized_text[0])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[66, 55, 26, 15, 7, 24, 28, 62, 12, 34, 45, 70, 38, 65, 74, 24, 38, 15, 15, 48, 30, 84, 27, 83, 30, 72, 55, 24, 49, 28, 58, 38, 67, 1, 24, 69, 45, 15, 1, 90, 15, 67, 59, 29, 61, 24, 38, 15, 15, 48, 30, 29, 61, 11, 76, 30, 66, 34, 43, 58, 84, 64, 9, 68, 58, 64, 9, 27, 56, 65, 58, 85, 19, 69, 28, 15, 90, 30, 72, 27, 29, 59, 30, 88, 57, 15, 90, 30, 19, 69, 80, 90, 26, 29, 28, 45, 29, 68, 61, 27, 79, 45, 43, 15, 24, 21, 65, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDIVHbHb25cs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_token = vocab_size\n",
        "end_token = vocab_size + 1\n",
        "        \n",
        "    \n",
        "class LM:\n",
        "    \n",
        "    def __init__(self, vocab_size, delta=1):\n",
        "        self.delta = delta\n",
        "        self.vocab_size = vocab_size + 2\n",
        "        self.proba = Counter() # TODO create array for storing 3-gram counters\n",
        "        \n",
        "    def infer(self, a, b, tau=1):\n",
        "        \"\"\"\n",
        "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for item in range(self.vocab_size):\n",
        "            result.append(self.get_proba(a, b, item, tau))\n",
        "        return result\n",
        "        \n",
        "    def get_proba(self, a, b, c, tau=1):\n",
        "        \"\"\"\n",
        "        get probability of 3-gram (a,b,c)\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        c: third token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        # TODO approbximate probability by counters\n",
        "        \n",
        "        def smooth(p, delta=self.delta, tau=tau):\n",
        "          return (p + delta) ** (1 / tau)\n",
        "        \n",
        "        P_abc = smooth(self.proba[(a, b, c)])\n",
        "        P_ab = sum([smooth(self.proba[(a,b,x)]) \n",
        "                    for x in range(self.vocab_size)])\n",
        "        \n",
        "        return math.log(P_abc / P_ab)\n",
        "    \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        train language model on text\n",
        "        text: list of lists\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        for txt in text:\n",
        "            for i in range(len(txt) - 2):\n",
        "                self.proba[txt[i], txt[i+1], txt[i+2]] += 1\n",
        "                                       \n",
        "        # self.proba = # TODO count 3-grams in the text\n",
        "    \n",
        "        return self\n",
        "    \n",
        "lm = LM(vocab_size, 1).fit(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vecEkHQj0PQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
        "    \"\"\"\n",
        "    generate sequence from language model *lm* conditioned on input_seq\n",
        "    input_seq: sequence of token ids for conditioning\n",
        "    lm: language model\n",
        "    max_len: max generated sequence length\n",
        "    k: size of beam\n",
        "    tau: temperature\n",
        "    \"\"\"\n",
        "    # TODO store in beam tuples of current sequences and their log probabilities\n",
        "    \n",
        "   \n",
        "    if len(input_seq) >= 3:\n",
        "      \n",
        "      prob = lm.get_proba(input_seq[0], input_seq[1] ,input_seq[2], tau)\n",
        "      \n",
        "      i = 1\n",
        "      while i + 2 < len(input_seq):\n",
        "        prob += lm.get_proba(input_seq[i], input_seq[i + 1] ,input_seq[i + 2], tau)\n",
        "        i += 1\n",
        "      \n",
        "      proba = enumerate(lm.infer(input_seq[-2], input_seq[-1], tau))\n",
        "      proba = sorted(proba, key=lambda x: x[1], reverse=True)[:k]\n",
        "      \n",
        "      beam = {}\n",
        "      for i in range(k):\n",
        "        keys = list(input_seq)\n",
        "        keys.append(proba[i][0])\n",
        "        keys = tuple(list(keys))\n",
        "        beam[keys] = proba[i][1] + prob\n",
        "        \n",
        "    elif len(input_seq) == 2:\n",
        "      beam = [item for item in lm.proba.keys() if item[0] == input_seq[-2] \\\n",
        "                and item[1] == input_seq[-1]]\n",
        "      beam = {item: lm.get_proba(item[0], item[1], item[2], tau) for item in beam}   \n",
        "    else:\n",
        "        beam = [item for item in lm.proba.keys() if item[0] == input_seq[0]]\n",
        "        beam = {item: lm.get_proba(item[0], item[1], item[2], tau) for item in beam}   \n",
        "            \n",
        "    beam = sorted(beam.items(), key= lambda x: x[1], reverse=True)[:k]\n",
        "    beam = {item[0] : item[1] for item in beam}\n",
        "    \n",
        "    for i in range(max_len):\n",
        "      if sum ([1 for item in beam.keys() if item[-1] != 0]) == 0:\n",
        "        break\n",
        "      candidates = []\n",
        "      candidates_proba = []\n",
        "      for snt, snt_proba in beam.items():          \n",
        "        key = list(snt)\n",
        "        if 0 in snt:\n",
        "          candidates.append(tuple(key))\n",
        "          candidates_proba.append(snt_proba)\n",
        "            # TODO process terminal token \n",
        "        else:   \n",
        "          proba = {i: j for i, j in enumerate(lm.infer(snt[-2], snt[-1]))} # probability vector of the next token\n",
        "          best_k = sorted(proba.items(), key=lambda x: x[1], reverse=True)\n",
        "          for item in best_k:\n",
        "            candidates_proba.append(item[1] + snt_proba)\n",
        "            key = list(snt)\n",
        "            key.append(item[0])\n",
        "            candidates.append(tuple(key))\n",
        "      beam = {candidates[a]: candidates_proba[a] for a in range(len(candidates))}\n",
        "      beam = sorted(beam.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "      beam = {i[0]: i[1] for i in beam}\n",
        "    #  print(i, beam)\n",
        "                   \n",
        "    return beam\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcC3hDJl25cw",
        "colab_type": "code",
        "outputId": "beae7a4a-e265-4780-ddcf-ac2a23063b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input1 = 'horse '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "\n",
        "for res in result.items():\n",
        "  print(bpe.decode(tuple([i for i in res[0] if i != '.'])), math.exp(res[1]))\n",
        "    "
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "horse with him. 0.00020842031405734197\n",
            "horse withought. 9.801724730010705e-05\n",
            "horse with himself. 1.4255371501910611e-05\n",
            "horse with with him  5.413034675990682e-06\n",
            "horse with himselves  4.7965209699688785e-06\n",
            "horse with himself wi 2.960555734184556e-06\n",
            "horse with himselvem 2.7151903715602943e-06\n",
            "horse with himself bu 1.6475059458537927e-06\n",
            "horse with with hims 1.4977529550640893e-06\n",
            "horse with himselve. 1.4001678571112323e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LHa-KbV25c0",
        "colab_type": "code",
        "outputId": "7b1bf2c1-d6a1-4419-b92e-3ceb76b2fd48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input1 = 'her'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "\n",
        "for res in result.items():\n",
        "  print(bpe.decode(tuple([i for i in res[0] if i != '.'])), math.exp(res[1]))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here with him. 9.796432005746645e-05\n",
            "here withought. 4.607129122268615e-05\n",
            "here with himself. 6.70048782273228e-06\n",
            "here with himselve 6.496754959727887e-06\n",
            "here with him with 5.861138738160363e-06\n",
            "here with himself and  5.8163297627770094e-06\n",
            "here with himself w 5.306125397621135e-06\n",
            "here with himself to  4.217689418621928e-06\n",
            "here with himself b 3.0612261909352725e-06\n",
            "here with himself in  2.8911580692166416e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh5TxF5U25c2",
        "colab_type": "code",
        "outputId": "9f6e3dc4-4fdf-4db3-fdc9-7a3a1d47fc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input1 = 'what'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "\n",
        "for res in result.items():\n",
        "  print(bpe.decode(tuple([i for i in res[0] if i != '.'])), math.exp(res[1]))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what. 0.0021381242623291935\n",
            "whated. 0.0007596152567260526\n",
            "whates. 0.0003372007833410722\n",
            "whats with him. 8.988461381837363e-07\n",
            "whatess with him. 7.531022157368436e-08\n",
            "whatess with himsel 7.44891126994022e-08\n",
            "whats with himself. 6.147858322169853e-08\n",
            "whats with himselve 5.9609285328086e-08\n",
            "whats with himself and  5.336622091194494e-08\n",
            "whats with himself w 4.868497346352874e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62a1Cpyu25c3",
        "colab_type": "code",
        "outputId": "e1a6b7c3-76cc-4b83-e7a7-224f76de1bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input1 = 'gun '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "\n",
        "for res in result.items():\n",
        "  print(bpe.decode(tuple([i for i in res[0] if i != '.'])), math.exp(res[1]))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gun with him. 6.955427047028502e-11\n",
            "gun withought. 3.2710430172313876e-11\n",
            "gun with himself. 4.757319216136916e-12\n",
            "gun with with him  1.8064442500326712e-12\n",
            "gun with himselves  1.6007005764794181e-12\n",
            "gun with himself wi 9.880001150999742e-13\n",
            "gun with himselvem 9.06116499900577e-13\n",
            "gun with himself bu 5.49807607178781e-13\n",
            "gun with with hims 4.998318643044331e-13\n",
            "gun with himselve. 4.672656515166748e-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHPvasVG25c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(snt, lm):\n",
        "    \"\"\"\n",
        "    snt: sequence of token ids\n",
        "    lm: language model\n",
        "    \"\"\"\n",
        "    i = 1\n",
        "    P = lm.get_proba(snt[0], snt[1], snt[2])\n",
        "    while i < len(snt) - 2:\n",
        "      P += lm.get_proba(snt[i], snt[i + 1], snt[i + 2])\n",
        "      i += 1\n",
        "    P = math.exp(P)\n",
        "    t = -1 *(1 / len(snt))\n",
        "\n",
        "    return P ** t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGpGGFDSnnhC",
        "colab_type": "code",
        "outputId": "8fc5c55c-7d41-4f58-a599-4fd2fcbf26b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "perplexity(tokenized_text[0], lm)\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.53881439694464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    }
  ]
}